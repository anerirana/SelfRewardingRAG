import numpy as np
import torch
import torch.nn as nn
from transformers import T5Tokenizer, T5ForConditionalGeneration
import DocumentRetrievalModel 
from constants import *

# Utilities
def exists(val):
    return val is not None

def default(val, default_value):
    return val if exists(val) else default_value

class LLM(nn.Module):
    def __init__(self, model_name):
        super(LLM, self).__init__()
        '''Retrival Augmented Generation model to generate responses for a given query 
        and a set of retieved documents
        '''
        self.tokenizer = T5Tokenizer.from_pretrained(model_name)
        self.model = T5ForConditionalGeneration.from_pretrained(model_name, device_map="auto", torch_dtype=torch.float16)

    def forward(self, prompt):
        '''Generate response for the given prompt

        Parameters:
        -----------
        prompt
            The prompt used as input to the generative model.
            Could be a RAG or QA or reward prompt.
        '''
        input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids.to("cuda")
        outputs = self.model.generate(input_ids)

        return self.tokenizer.decode(outputs[0])

    def train(self, training_dataset, batch_size=32, num_epochs=3):
        '''Train the RAGModel on the given training_dataset

        Parameters:
        -----------
        training_dataset
            Contains the training data for query augemntation or reward generation or answer generator
        '''
        
        pass

class RAGPipeline:
    def __init__(self, config: dict):
        '''Executes each block of RAGPipeline to train query augmentation and RAG models

        Parameters:
        -----------
        qa_prompt_template
            The query augmentation prompt template to generate n-1 similar queries to a given query
        rag_prompt_template
            The prompt template to use for generating responses
        reward_prompt_template
            The prompt template to self reward responses generated by the RAGModel
        num_documents
            Numnber of docuemts to retrieve per query by the Document Retrieval Model 
        '''
        self.reward_prompt_template = default(config.get('RewardPromptTemplate'), 'Default Prompt')
        self.num_documents = default(config.get('NumberOfRetrievedDocuments'), 'Default Prompt')
        self.m = default(config.get('NumberOfQuerySets'), 'Default Prompt')

    def train(self, original_query):
        '''Executes a training loop of the RAGPipeline

        Parameters:
        -----------
        original_query
            The original query to generate responses for
        '''
        # Create instances of required models
        language_model = LLM()
        document_retrieval_model = DocumentRetrievalModel()   
        pp_generator = PreferencePairGenerator(language_model)

        aug_queries = np.asarray([])
        all_documents = np.asarray([])
        top_documents = np.asarray([])
        all_responses = np.asarray([])
        all_rewards = np.asarray([])
        contributing_documents = {}
        first_pps = []

        for i in range(self.m):
            queries = self.extract_query_samples(language_model, original_query, n=5)

            top_k_docs, all_docs = document_retrieval_model(queries)

            rag_prompt = RAG_PROMPT.format(original_query = original_query, documents = top_k_docs)

            #TODO: Need to sample from the language model to get l answers
            responses = language_model.forward(rag_prompt)

            responses, contri_docs = self.parser(responses,i)
                 
            rewards = [language_model(self.create_reward_prompt_template(response)) for response in responses.keys]

            pp1 = pp_generator.generateFirstPP(rag_prompt, responses.keys, rewards)

            first_pps.append(pp1)

            aug_queries = np.vstack((aug_queries, queries))
            all_documents = np.vstack((all_documents, all_docs))
            top_documents = np.vstack((top_documents, top_k_docs))
            all_responses = np.vstack((all_responses, responses))
            all_rewards = np.vstack((all_rewards, rewards))
            contributing_documents.update(contri_docs)
        
        pp2 = pp_generator.generateSecondPP(self.qa_prompt_template.format(original_query), aug_queries, all_documents, top_documents, all_rewards, contributing_documents)
        
        #TODO: load pp1 and pp2 in a dataset loader for training
        language_model.train()
        
            
    def parser(self, responses, index):
        return {}
    
    def create_reward_prompt_template(self, response):
        return ''
    
    def extract_query_samples(self, language_model, original_query, n=5):
        '''
        Extracts query samples from the language model
        '''

        qa_prompt = self.qa_prompt_template.format(n, original_query)
        max_tries = 5
        response = ""
        j = 0
        sanity_check = False

        while j < max_tries and sanity_check == False:
            j += 1
            response = language_model.forward(qa_prompt)
            sanity_check = True
            for i in range(1, n+1):
                if f"{i}." not in response:
                    sanity_check = False
                    break
        
        queries = response.split(qa_prompt)[-1] #Remove the prompt from the response
        queries = queries.replace("<s>", "").replace("</s>", "").replace("<pad>", "") #Remove special tokens
        
        for i in range(1, n+1):
            queries = queries.replace(f"{i}.", "")
        
        queries = queries.strip().split("\n") #Split the response into a list of queries

        return queries


class PreferencePairGenerator:
    def __init__(self, rag_model: LLM):
        '''Generate preference pairs for a training loop of RAG pipeline

        Parameters:
        -----------
        rag_model
            RAG model to generate responses and corresponding rewards
        '''
        self.rag_model = rag_model
    
    def get_document_rewards(rho, winning_answers, top_documents, contributing_documents):
        reward_docs = np.asarray([])
        for i, top_docs in enumerate(top_documents):
            if rho[i] < 0:
                reward_docs = np.vstack(reward_docs, np.full(len(top_docs), np.nan()))
            else:
                rewards = np.array([rho[i] if doc in contributing_documents[i][winning_answers[i]] else 0 for doc in top_docs])
                reward_docs = np.vstack(reward_docs, rewards)

        reward_docs = np.array(reward_docs)
        return reward_docs     
    
    def get_augment_query_rewards(all_documents, document_rewards, top_documents, rho, aug_queries):
        rewards_aug_query = []
        
        for i in range(aug_queries.shape[0]):
            if rho[i] < 0:
                # Set the entire row to NaN if rho[i] is negative
                rewards_aug_query.append([np.nan] * aug_queries.shape[1])
            else:
                z = []
                for j in range(aug_queries.shape[1]):
                    p = 0
                    for h, top_doc in enumerate(top_documents[i]):
                        if top_doc in all_documents[i][j]:
                            index_in_all_docs = np.where(all_documents[i][j] == top_doc)
                            p += document_rewards[i][h] / (index_in_all_docs + 1)
                    if p > 0:
                      z.append(p)
                    else:
                        z.append(np.nan)
                rewards_aug_query.append(z)
        
        # Convert list of lists to a NumPy array
        rewards_aug_query = np.array(rewards_aug_query)

        return rewards_aug_query
    
    def agg_query_rewards(self, aug_query_rewards, aug_queries):
        unique_queries = np.unique(aug_queries)  
        agg_query_rewards = {}

        for query in unique_queries:
            indices = np.where(aug_queries == query)
            avg_reward = np.mean(aug_query_rewards[indices], axis=0)
            if avg_reward > 0:
              agg_query_rewards[query] = avg_reward
        return agg_query_rewards

    def generateFirstPP(self, prompt, responses, rewards):
        '''Generates the first preference pair matrix
        '''
        responses = np.asarray(responses)
        rewards = np.asarray(rewards)

        max_idx = np.argmax(rewards)
        min_idx = np.argmin(rewards)
        
        return (prompt, responses[max_idx], responses[min_idx])  # Placeholder for a matrix

    def generateSecondPP(self, qa_prompt, aug_queries, all_documents, top_documents, all_rewards, contributing_documents):
        '''generate the second preference pair matrix
        
        '''
        rho = np.max(all_rewards, axis=1)
        # rho normalize
        rho = rho - np.mean(rho)
        winning_answers = np.argmax(all_rewards, axis=1)
        document_rewards = self.get_document_rewards(rho, winning_answers, all_documents,top_documents,contributing_documents)
        aug_query_rewards = self.get_augment_query_rewards(all_documents, document_rewards, top_documents,rho,aug_queries)
        agg_query_rewards = self.agg_query_rewards(aug_query_rewards, aug_queries)

        unique_queries = len(agg_query_rewards.keys())
        pairs = []
        for i in range(len(unique_queries)):
            for j in range(i + 1, len(unique_queries)):
                query1, query2 = unique_queries[i], unique_queries[i]
                reward1, reward2 = agg_query_rewards.get(query1), agg_query_rewards.get(query2)
                if reward1 > reward2:
                    pairs.append((qa_prompt, query1, query2))
                else:
                    pairs.append((qa_prompt, query2, query1))
        return pairs